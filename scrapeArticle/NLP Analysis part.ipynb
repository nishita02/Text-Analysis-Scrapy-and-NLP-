{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8742749c",
   "metadata": {},
   "source": [
    "### DATA CLEANING AND TRANSFORMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbfb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a31f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scrapedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "172deacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“If anything kills over 10 million people in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>AI is rapidly evolving in the employment secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Will machine replace the human in the future o...</td>\n",
       "      <td>Where is this disruptive technology taking us?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>“Machine intelligence is the last invention th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>146</td>\n",
       "      <td>Blockchain for Payments</td>\n",
       "      <td>between having a tight budget and being seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>149</td>\n",
       "      <td>Business Analytics In The Healthcare Industry</td>\n",
       "      <td>Analytics is a statistical scientific process ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>141</td>\n",
       "      <td>Impact of COVID-19 (Coronavirus) on the Indian...</td>\n",
       "      <td>The\\ncorona outbreak has hit us hard. With the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>148</td>\n",
       "      <td>Big Data Analytics in Healthcare</td>\n",
       "      <td>Quality and affordable healthcare is a vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>150</td>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "      <td>To begin with I shall first like to explain wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     url_id                                      article_title  \\\n",
       "0        37                                                NaN   \n",
       "1        39  What Jobs Will Robots Take From Humans in The ...   \n",
       "2        42  Will machine replace the human in the future o...   \n",
       "3        38   What if the Creation is Taking Over the Creator?   \n",
       "4        41                Will AI Replace Us or Work With Us?   \n",
       "..      ...                                                ...   \n",
       "106     146                            Blockchain for Payments   \n",
       "107     149      Business Analytics In The Healthcare Industry   \n",
       "108     141  Impact of COVID-19 (Coronavirus) on the Indian...   \n",
       "109     148                   Big Data Analytics in Healthcare   \n",
       "110     150  Challenges and Opportunities of Big Data in He...   \n",
       "\n",
       "                                          article_text  \n",
       "0    “If anything kills over 10 million people in t...  \n",
       "1    AI is rapidly evolving in the employment secto...  \n",
       "2    Where is this disruptive technology taking us?...  \n",
       "3    Human minds, a fascination in itself carrying ...  \n",
       "4    “Machine intelligence is the last invention th...  \n",
       "..                                                 ...  \n",
       "106  between having a tight budget and being seriou...  \n",
       "107  Analytics is a statistical scientific process ...  \n",
       "108  The\\ncorona outbreak has hit us hard. With the...  \n",
       "109  Quality and affordable healthcare is a vision ...  \n",
       "110  To begin with I shall first like to explain wh...  \n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41c3202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“If anything kills over 10 million people in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>AI is rapidly evolving in the employment secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>“Anything that could give rise to smarter-than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>“Machine intelligence is the last invention th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>146</td>\n",
       "      <td>Blockchain for Payments</td>\n",
       "      <td>between having a tight budget and being seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>147</td>\n",
       "      <td>The future of Investing</td>\n",
       "      <td>An investment is a resource or thing procured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>148</td>\n",
       "      <td>Big Data Analytics in Healthcare</td>\n",
       "      <td>Quality and affordable healthcare is a vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>149</td>\n",
       "      <td>Business Analytics In The Healthcare Industry</td>\n",
       "      <td>Analytics is a statistical scientific process ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>150</td>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "      <td>To begin with I shall first like to explain wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     url_id                                      article_title  \\\n",
       "0        37                                                NaN   \n",
       "3        38   What if the Creation is Taking Over the Creator?   \n",
       "1        39  What Jobs Will Robots Take From Humans in The ...   \n",
       "5        40  Will Machine Replace The Human in the Future o...   \n",
       "4        41                Will AI Replace Us or Work With Us?   \n",
       "..      ...                                                ...   \n",
       "106     146                            Blockchain for Payments   \n",
       "103     147                            The future of Investing   \n",
       "109     148                   Big Data Analytics in Healthcare   \n",
       "107     149      Business Analytics In The Healthcare Industry   \n",
       "110     150  Challenges and Opportunities of Big Data in He...   \n",
       "\n",
       "                                          article_text  \n",
       "0    “If anything kills over 10 million people in t...  \n",
       "3    Human minds, a fascination in itself carrying ...  \n",
       "1    AI is rapidly evolving in the employment secto...  \n",
       "5    “Anything that could give rise to smarter-than...  \n",
       "4    “Machine intelligence is the last invention th...  \n",
       "..                                                 ...  \n",
       "106  between having a tight budget and being seriou...  \n",
       "103  An investment is a resource or thing procured ...  \n",
       "109  Quality and affordable healthcare is a vision ...  \n",
       "107  Analytics is a statistical scientific process ...  \n",
       "110  To begin with I shall first like to explain wh...  \n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values('url_id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c9c817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the column names of the df so it can match with the output frame\n",
    "\n",
    "df.columns = ['URL_ID', 'ARTICLE_TITLE', 'ARTICLE_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b15f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_excel('../Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a27775f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               NaN             NaN             NaN                 NaN   \n",
       "1               NaN             NaN             NaN                 NaN   \n",
       "2               NaN             NaN             NaN                 NaN   \n",
       "3               NaN             NaN             NaN                 NaN   \n",
       "4               NaN             NaN             NaN                 NaN   \n",
       "..              ...             ...             ...                 ...   \n",
       "109             NaN             NaN             NaN                 NaN   \n",
       "110             NaN             NaN             NaN                 NaN   \n",
       "111             NaN             NaN             NaN                 NaN   \n",
       "112             NaN             NaN             NaN                 NaN   \n",
       "113             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                    NaN                          NaN        NaN   \n",
       "1                    NaN                          NaN        NaN   \n",
       "2                    NaN                          NaN        NaN   \n",
       "3                    NaN                          NaN        NaN   \n",
       "4                    NaN                          NaN        NaN   \n",
       "..                   ...                          ...        ...   \n",
       "109                  NaN                          NaN        NaN   \n",
       "110                  NaN                          NaN        NaN   \n",
       "111                  NaN                          NaN        NaN   \n",
       "112                  NaN                          NaN        NaN   \n",
       "113                  NaN                          NaN        NaN   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                 NaN                 NaN         NaN   \n",
       "1                                 NaN                 NaN         NaN   \n",
       "2                                 NaN                 NaN         NaN   \n",
       "3                                 NaN                 NaN         NaN   \n",
       "4                                 NaN                 NaN         NaN   \n",
       "..                                ...                 ...         ...   \n",
       "109                               NaN                 NaN         NaN   \n",
       "110                               NaN                 NaN         NaN   \n",
       "111                               NaN                 NaN         NaN   \n",
       "112                               NaN                 NaN         NaN   \n",
       "113                               NaN                 NaN         NaN   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                  NaN                NaN              NaN  \n",
       "1                  NaN                NaN              NaN  \n",
       "2                  NaN                NaN              NaN  \n",
       "3                  NaN                NaN              NaN  \n",
       "4                  NaN                NaN              NaN  \n",
       "..                 ...                ...              ...  \n",
       "109                NaN                NaN              NaN  \n",
       "110                NaN                NaN              NaN  \n",
       "111                NaN                NaN              NaN  \n",
       "112                NaN                NaN              NaN  \n",
       "113                NaN                NaN              NaN  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7164d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to perform right join on the datasets to join the both on the basis of url id\n",
    "# there are 3 rows missing in our scrapedData because of 404 error\n",
    "\n",
    "final = pd.merge(df,output, on='URL_ID', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9a715e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>ARTICLE_TITLE</th>\n",
       "      <th>ARTICLE_TEXT</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://insights.blackcoffer.com/how-neural-ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://insights.blackcoffer.com/covid-19-envi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://insights.blackcoffer.com/ensuring-grow...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID ARTICLE_TITLE ARTICLE_TEXT  \\\n",
       "7        44           NaN          NaN   \n",
       "20       57           NaN          NaN   \n",
       "107     144           NaN          NaN   \n",
       "\n",
       "                                                   URL  POSITIVE SCORE  \\\n",
       "7    https://insights.blackcoffer.com/how-neural-ne...             NaN   \n",
       "20   https://insights.blackcoffer.com/covid-19-envi...             NaN   \n",
       "107  https://insights.blackcoffer.com/ensuring-grow...             NaN   \n",
       "\n",
       "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "7               NaN             NaN                 NaN                  NaN   \n",
       "20              NaN             NaN                 NaN                  NaN   \n",
       "107             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "7                            NaN        NaN                               NaN   \n",
       "20                           NaN        NaN                               NaN   \n",
       "107                          NaN        NaN                               NaN   \n",
       "\n",
       "     COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "7                   NaN         NaN                NaN                NaN   \n",
       "20                  NaN         NaN                NaN                NaN   \n",
       "107                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "     AVG WORD LENGTH processed_data  \n",
       "7                NaN           None  \n",
       "20               NaN           None  \n",
       "107              NaN           None  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some of the articles' text could not be scraped because of 404 error\n",
    "\n",
    "final[final['ARTICLE_TEXT'].isnull()==True] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddce26",
   "metadata": {},
   "source": [
    "### NLP - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfdbefe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ------------                             0.5/1.5 MB 10.0 MB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-win_amd64.whl (268 kB)\n",
      "                                              0.0/268.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 268.0/268.0 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "                                              0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in e:\\blackcoffer\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b35884b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3578bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ngunj/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1959b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import regular expression to remove the special characters\n",
    "\n",
    "import re\n",
    "\n",
    "# for tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b55d6",
   "metadata": {},
   "source": [
    "### Extracting custom stopwords from the folder given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9b0d5b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " '|',\n",
       " 'Afghanistan',\n",
       " 'ARIARY',\n",
       " '|',\n",
       " 'Madagascar',\n",
       " 'BAHT',\n",
       " '|',\n",
       " 'Thailand',\n",
       " 'BALBOA',\n",
       " '|',\n",
       " 'Panama',\n",
       " 'BIRR',\n",
       " '|',\n",
       " 'Ethiopia',\n",
       " 'BOLIVAR',\n",
       " '|',\n",
       " 'Venezuela',\n",
       " 'BOLIVIANO',\n",
       " '|',\n",
       " 'Bolivia',\n",
       " 'CEDI',\n",
       " '|',\n",
       " 'Ghana',\n",
       " 'COLON',\n",
       " '|',\n",
       " 'Costa',\n",
       " 'Rica',\n",
       " 'CÓRDOBA',\n",
       " '|',\n",
       " 'Nicaragua',\n",
       " 'DALASI',\n",
       " '|',\n",
       " 'Gambia',\n",
       " 'DENAR',\n",
       " '|',\n",
       " 'Macedonia',\n",
       " '(Former',\n",
       " 'Yug.',\n",
       " 'Rep.)',\n",
       " 'DINAR',\n",
       " '|',\n",
       " 'Algeria',\n",
       " 'DIRHAM',\n",
       " '|',\n",
       " 'Morocco',\n",
       " 'DOBRA',\n",
       " '|',\n",
       " 'São',\n",
       " 'Tom',\n",
       " 'and',\n",
       " 'Príncipe',\n",
       " 'DONG',\n",
       " '|',\n",
       " 'Vietnam',\n",
       " 'DRAM',\n",
       " '|',\n",
       " 'Armenia',\n",
       " 'ESCUDO',\n",
       " '|',\n",
       " 'Cape',\n",
       " 'Verde',\n",
       " 'EURO',\n",
       " '|',\n",
       " 'Belgium',\n",
       " 'FLORIN',\n",
       " '|',\n",
       " 'Aruba',\n",
       " 'FORINT',\n",
       " '|',\n",
       " 'Hungary',\n",
       " 'GOURDE',\n",
       " '|',\n",
       " 'Haiti',\n",
       " 'GUARANI',\n",
       " '|',\n",
       " 'Paraguay',\n",
       " 'GULDEN',\n",
       " '|',\n",
       " 'Netherlands',\n",
       " 'Antilles',\n",
       " 'HRYVNIA',\n",
       " '|',\n",
       " 'Ukraine',\n",
       " 'KINA',\n",
       " '|',\n",
       " 'Papua',\n",
       " 'New',\n",
       " 'Guinea',\n",
       " 'KIP',\n",
       " '|',\n",
       " 'Laos',\n",
       " 'KONVERTIBILNA',\n",
       " 'MARKA',\n",
       " '|',\n",
       " 'Bosnia-Herzegovina',\n",
       " 'KORUNA',\n",
       " '|',\n",
       " 'Czech',\n",
       " 'Republic',\n",
       " 'KRONA',\n",
       " '|',\n",
       " 'Sweden',\n",
       " 'KRONE',\n",
       " '|',\n",
       " 'Denmark',\n",
       " 'KROON',\n",
       " '|',\n",
       " 'Estonia',\n",
       " 'KUNA',\n",
       " '|',\n",
       " 'Croatia',\n",
       " 'KWACHA',\n",
       " '|',\n",
       " 'Zambia',\n",
       " 'KWANZA',\n",
       " '|',\n",
       " 'Angola',\n",
       " 'KYAT',\n",
       " '|',\n",
       " 'Myanmar',\n",
       " 'LARI',\n",
       " '|',\n",
       " 'Georgia',\n",
       " 'LATS',\n",
       " '|',\n",
       " 'Latvia',\n",
       " 'LEK',\n",
       " '|',\n",
       " 'Albania',\n",
       " 'LEMPIRA',\n",
       " '|',\n",
       " 'Honduras',\n",
       " 'LEONE',\n",
       " '|',\n",
       " 'Sierra',\n",
       " 'Leone',\n",
       " 'LEU',\n",
       " '|',\n",
       " 'Romania',\n",
       " 'LEV',\n",
       " '|',\n",
       " 'Bulgaria',\n",
       " 'LILANGENI',\n",
       " '|',\n",
       " 'Swaziland',\n",
       " 'LIRA',\n",
       " '|',\n",
       " 'Lebanon',\n",
       " 'LITAS',\n",
       " '|',\n",
       " 'Lithuania',\n",
       " 'LOTI',\n",
       " '|',\n",
       " 'Lesotho',\n",
       " 'MANAT',\n",
       " '|',\n",
       " 'Azerbaijan',\n",
       " 'METICAL',\n",
       " '|',\n",
       " 'Mozambique',\n",
       " 'NAIRA',\n",
       " '|',\n",
       " 'Nigeria',\n",
       " 'NAKFA',\n",
       " '|',\n",
       " 'Eritrea',\n",
       " 'NEW',\n",
       " 'LIRA',\n",
       " '|',\n",
       " 'Turkey',\n",
       " 'NEW',\n",
       " 'SHEQEL',\n",
       " '|',\n",
       " 'Israel',\n",
       " 'NGULTRUM',\n",
       " '|',\n",
       " 'Bhutan',\n",
       " 'NUEVO',\n",
       " 'SOL',\n",
       " '|',\n",
       " 'Peru',\n",
       " 'OUGUIYA',\n",
       " '|',\n",
       " 'Mauritania',\n",
       " 'PATACA',\n",
       " '|',\n",
       " 'Macau',\n",
       " 'PESO',\n",
       " '|',\n",
       " 'Mexico',\n",
       " 'POUND',\n",
       " '|',\n",
       " 'Egypt',\n",
       " 'PULA',\n",
       " '|',\n",
       " 'Botswana',\n",
       " 'QUETZAL',\n",
       " '|',\n",
       " 'Guatemala',\n",
       " 'RAND',\n",
       " '|',\n",
       " 'South',\n",
       " 'Africa',\n",
       " 'REAL',\n",
       " '|',\n",
       " 'Brazil',\n",
       " 'RENMINBI',\n",
       " '|',\n",
       " 'China',\n",
       " 'RIAL',\n",
       " '|',\n",
       " 'Iran',\n",
       " 'RIEL',\n",
       " '|',\n",
       " 'Cambodia',\n",
       " 'RINGGIT',\n",
       " '|',\n",
       " 'Malaysia',\n",
       " 'RIYAL',\n",
       " '|',\n",
       " 'Saudi',\n",
       " 'Arabia',\n",
       " 'RUBLE',\n",
       " '|',\n",
       " 'Russia',\n",
       " 'RUFIYAA',\n",
       " '|',\n",
       " 'Maldives',\n",
       " 'RUPEE',\n",
       " '|',\n",
       " 'India',\n",
       " 'RUPEE',\n",
       " '|',\n",
       " 'Pakistan',\n",
       " 'RUPIAH',\n",
       " '|',\n",
       " 'Indonesia',\n",
       " 'SHILLING',\n",
       " '|',\n",
       " 'Uganda',\n",
       " 'SOM',\n",
       " '|',\n",
       " 'Uzbekistan',\n",
       " 'SOMONI',\n",
       " '|',\n",
       " 'Tajikistan',\n",
       " 'SPECIAL',\n",
       " 'DRAWING',\n",
       " 'RIGHTS',\n",
       " '|',\n",
       " 'International',\n",
       " 'Monetary',\n",
       " 'Fund',\n",
       " 'TAKA',\n",
       " '|',\n",
       " 'Bangladesh',\n",
       " 'TALA',\n",
       " '|',\n",
       " 'Western',\n",
       " 'Samoa',\n",
       " 'TENGE',\n",
       " '|',\n",
       " 'Kazakhstan',\n",
       " 'TUGRIK',\n",
       " '|',\n",
       " 'Mongolia',\n",
       " 'VATU',\n",
       " '|',\n",
       " 'Vanuatu',\n",
       " 'WON',\n",
       " '|',\n",
       " 'Korea,',\n",
       " 'South',\n",
       " 'YEN',\n",
       " '|',\n",
       " 'Japan',\n",
       " 'ZLOTY',\n",
       " '|',\n",
       " 'Poland',\n",
       " 'HUNDRED',\n",
       " '|',\n",
       " 'Denominations',\n",
       " 'THOUSAND',\n",
       " 'MILLION',\n",
       " 'BILLION',\n",
       " 'TRILLION',\n",
       " 'DATE',\n",
       " '|',\n",
       " 'Time',\n",
       " 'related',\n",
       " 'ANNUAL',\n",
       " 'ANNUALLY',\n",
       " 'ANNUM',\n",
       " 'YEAR',\n",
       " 'YEARLY',\n",
       " 'QUARTER',\n",
       " 'QUARTERLY',\n",
       " 'QTR',\n",
       " 'MONTH',\n",
       " 'MONTHLY',\n",
       " 'WEEK',\n",
       " 'WEEKLY',\n",
       " 'DAY',\n",
       " 'DAILY',\n",
       " 'JANUARY',\n",
       " '|',\n",
       " 'Calendar',\n",
       " 'FEBRUARY',\n",
       " 'MARCH',\n",
       " 'APRIL',\n",
       " 'MAY',\n",
       " 'JUNE',\n",
       " 'JULY',\n",
       " 'AUGUST',\n",
       " 'SEPTEMBER',\n",
       " 'OCTOBER',\n",
       " 'NOVEMBER',\n",
       " 'DECEMBER',\n",
       " 'JAN',\n",
       " 'FEB',\n",
       " 'MAR',\n",
       " 'APR',\n",
       " 'MAY',\n",
       " 'JUN',\n",
       " 'JUL',\n",
       " 'AUG',\n",
       " 'SEP',\n",
       " 'SEPT',\n",
       " 'OCT',\n",
       " 'NOV',\n",
       " 'DEC',\n",
       " 'MONDAY',\n",
       " 'TUESDAY',\n",
       " 'WEDNESDAY',\n",
       " 'THURSDAY',\n",
       " 'FRIDAY',\n",
       " 'SATURDAY',\n",
       " 'SUNDAY',\n",
       " 'ONE',\n",
       " '|',\n",
       " 'Numbers',\n",
       " 'TWO',\n",
       " 'THREE',\n",
       " 'FOUR',\n",
       " 'FIVE',\n",
       " 'SIX',\n",
       " 'SEVEN',\n",
       " 'EIGHT',\n",
       " 'NINE',\n",
       " 'TEN',\n",
       " 'ELEVEN',\n",
       " 'TWELVE',\n",
       " 'THIRTEEN',\n",
       " 'FOURTEEN',\n",
       " 'FIFTEEN',\n",
       " 'SIXTEEN',\n",
       " 'SEVENTEEN',\n",
       " 'EIGHTEEN',\n",
       " 'NINETEEN',\n",
       " 'TWENTY',\n",
       " 'THIRTY',\n",
       " 'FORTY',\n",
       " 'FIFTY',\n",
       " 'SIXTY',\n",
       " 'SEVENTY',\n",
       " 'EIGHTY',\n",
       " 'NINETY',\n",
       " 'FIRST',\n",
       " 'SECOND',\n",
       " 'THIRD',\n",
       " 'FOURTH',\n",
       " 'FIFTH',\n",
       " 'SIXTH',\n",
       " 'SEVENTH',\n",
       " 'EIGHTH',\n",
       " 'NINTH',\n",
       " 'TENTH',\n",
       " 'I',\n",
       " '|',\n",
       " 'Roman',\n",
       " 'numerals',\n",
       " 'II',\n",
       " 'III',\n",
       " 'IV',\n",
       " 'V',\n",
       " 'VI',\n",
       " 'VII',\n",
       " 'VIII',\n",
       " 'IX',\n",
       " 'X',\n",
       " 'XI',\n",
       " 'XII',\n",
       " 'XIII',\n",
       " 'XIV',\n",
       " 'XV',\n",
       " 'XVI',\n",
       " 'XVII',\n",
       " 'XVIII',\n",
       " 'XIX',\n",
       " 'XX',\n",
       " 'ABOUT',\n",
       " 'ABOVE',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AMONG',\n",
       " 'AN',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEEN',\n",
       " 'BEFORE',\n",
       " 'BEING',\n",
       " 'BELOW',\n",
       " 'BETWEEN',\n",
       " 'BOTH',\n",
       " 'BUT',\n",
       " 'BY',\n",
       " 'CAN',\n",
       " 'DID',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DOWN',\n",
       " 'DURING',\n",
       " 'EACH',\n",
       " 'FEW',\n",
       " 'FOR',\n",
       " 'FROM',\n",
       " 'FURTHER',\n",
       " 'HAD',\n",
       " 'HAS',\n",
       " 'HAVE',\n",
       " 'HAVING',\n",
       " 'HE',\n",
       " 'HER',\n",
       " 'HERE',\n",
       " 'HERS',\n",
       " 'HERSELF',\n",
       " 'HIM',\n",
       " 'HIMSELF',\n",
       " 'HIS',\n",
       " 'HOW',\n",
       " 'IF',\n",
       " 'IN',\n",
       " 'INTO',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'ITS',\n",
       " 'ITSELF',\n",
       " 'JUST',\n",
       " 'ME',\n",
       " 'MORE',\n",
       " 'MOST',\n",
       " 'MY',\n",
       " 'MYSELF',\n",
       " 'NO',\n",
       " 'NOR',\n",
       " 'NOT',\n",
       " 'NOW',\n",
       " 'OF',\n",
       " 'OFF',\n",
       " 'ON',\n",
       " 'ONCE',\n",
       " 'ONLY',\n",
       " 'OR',\n",
       " 'OTHER',\n",
       " 'OUR',\n",
       " 'OURS',\n",
       " 'OURSELVES',\n",
       " 'OUT',\n",
       " 'OVER',\n",
       " 'OWN',\n",
       " 'SAME',\n",
       " 'SHE',\n",
       " 'SHOULD',\n",
       " 'SO',\n",
       " 'SOME',\n",
       " 'SUCH',\n",
       " 'THAN',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'THEIR',\n",
       " 'THEIRS',\n",
       " 'THEM',\n",
       " 'THEMSELVES',\n",
       " 'THEN',\n",
       " 'THERE',\n",
       " 'THESE',\n",
       " 'THEY',\n",
       " 'THIS',\n",
       " 'THOSE',\n",
       " 'THROUGH',\n",
       " 'TO',\n",
       " 'TOO',\n",
       " 'UNDER',\n",
       " 'UNTIL',\n",
       " 'UP',\n",
       " 'VERY',\n",
       " 'WAS',\n",
       " 'WE',\n",
       " 'WERE',\n",
       " 'WHAT',\n",
       " 'WHEN',\n",
       " 'WHERE',\n",
       " 'WHICH',\n",
       " 'WHILE',\n",
       " 'WHO',\n",
       " 'WHOM',\n",
       " 'WHY',\n",
       " 'WITH',\n",
       " 'YOU',\n",
       " 'YOUR',\n",
       " 'YOURS',\n",
       " 'YOURSELF',\n",
       " 'YOURSELVES',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the list of stopwords\n",
    "\n",
    "import os\n",
    "\n",
    "custom_stopwords = []\n",
    "\n",
    "stopwords_folder = '../StopWords'\n",
    "\n",
    "for stopwords_file in os.listdir(stopwords_folder):\n",
    "    filepath = os.path.join(stopwords_folder,stopwords_file)\n",
    "    if(os.path.isfile(filepath)):                              # to check is if the file is present in that location\n",
    "        with open(filepath, 'r') as file:\n",
    "            words = file.read().split()\n",
    "            custom_stopwords.extend(words)\n",
    "            \n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8083dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ernst',\n",
       " 'young',\n",
       " 'deloitte',\n",
       " 'touche',\n",
       " 'kpmg',\n",
       " 'pricewaterhousecoopers',\n",
       " 'pricewaterhouse',\n",
       " 'coopers',\n",
       " 'afghani',\n",
       " 'afghanistan',\n",
       " 'ariary',\n",
       " 'madagascar',\n",
       " 'baht',\n",
       " 'thailand',\n",
       " 'balboa',\n",
       " 'panama',\n",
       " 'birr',\n",
       " 'ethiopia',\n",
       " 'bolivar',\n",
       " 'venezuela',\n",
       " 'boliviano',\n",
       " 'bolivia',\n",
       " 'cedi',\n",
       " 'ghana',\n",
       " 'colon',\n",
       " 'costa',\n",
       " 'rica',\n",
       " 'córdoba',\n",
       " 'nicaragua',\n",
       " 'dalasi',\n",
       " 'gambia',\n",
       " 'denar',\n",
       " 'macedonia',\n",
       " '(former',\n",
       " 'yug.',\n",
       " 'rep.)',\n",
       " 'dinar',\n",
       " 'algeria',\n",
       " 'dirham',\n",
       " 'morocco',\n",
       " 'dobra',\n",
       " 'são',\n",
       " 'tom',\n",
       " 'and',\n",
       " 'príncipe',\n",
       " 'dong',\n",
       " 'vietnam',\n",
       " 'dram',\n",
       " 'armenia',\n",
       " 'escudo',\n",
       " 'cape',\n",
       " 'verde',\n",
       " 'euro',\n",
       " 'belgium',\n",
       " 'florin',\n",
       " 'aruba',\n",
       " 'forint',\n",
       " 'hungary',\n",
       " 'gourde',\n",
       " 'haiti',\n",
       " 'guarani',\n",
       " 'paraguay',\n",
       " 'gulden',\n",
       " 'netherlands',\n",
       " 'antilles',\n",
       " 'hryvnia',\n",
       " 'ukraine',\n",
       " 'kina',\n",
       " 'papua',\n",
       " 'new',\n",
       " 'guinea',\n",
       " 'kip',\n",
       " 'laos',\n",
       " 'konvertibilna',\n",
       " 'marka',\n",
       " 'bosnia-herzegovina',\n",
       " 'koruna',\n",
       " 'czech',\n",
       " 'republic',\n",
       " 'krona',\n",
       " 'sweden',\n",
       " 'krone',\n",
       " 'denmark',\n",
       " 'kroon',\n",
       " 'estonia',\n",
       " 'kuna',\n",
       " 'croatia',\n",
       " 'kwacha',\n",
       " 'zambia',\n",
       " 'kwanza',\n",
       " 'angola',\n",
       " 'kyat',\n",
       " 'myanmar',\n",
       " 'lari',\n",
       " 'georgia',\n",
       " 'lats',\n",
       " 'latvia',\n",
       " 'lek',\n",
       " 'albania',\n",
       " 'lempira',\n",
       " 'honduras',\n",
       " 'leone',\n",
       " 'sierra',\n",
       " 'leone',\n",
       " 'leu',\n",
       " 'romania',\n",
       " 'lev',\n",
       " 'bulgaria',\n",
       " 'lilangeni',\n",
       " 'swaziland',\n",
       " 'lira',\n",
       " 'lebanon',\n",
       " 'litas',\n",
       " 'lithuania',\n",
       " 'loti',\n",
       " 'lesotho',\n",
       " 'manat',\n",
       " 'azerbaijan',\n",
       " 'metical',\n",
       " 'mozambique',\n",
       " 'naira',\n",
       " 'nigeria',\n",
       " 'nakfa',\n",
       " 'eritrea',\n",
       " 'new',\n",
       " 'lira',\n",
       " 'turkey',\n",
       " 'new',\n",
       " 'sheqel',\n",
       " 'israel',\n",
       " 'ngultrum',\n",
       " 'bhutan',\n",
       " 'nuevo',\n",
       " 'sol',\n",
       " 'peru',\n",
       " 'ouguiya',\n",
       " 'mauritania',\n",
       " 'pataca',\n",
       " 'macau',\n",
       " 'peso',\n",
       " 'mexico',\n",
       " 'pound',\n",
       " 'egypt',\n",
       " 'pula',\n",
       " 'botswana',\n",
       " 'quetzal',\n",
       " 'guatemala',\n",
       " 'rand',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'real',\n",
       " 'brazil',\n",
       " 'renminbi',\n",
       " 'china',\n",
       " 'rial',\n",
       " 'iran',\n",
       " 'riel',\n",
       " 'cambodia',\n",
       " 'ringgit',\n",
       " 'malaysia',\n",
       " 'riyal',\n",
       " 'saudi',\n",
       " 'arabia',\n",
       " 'ruble',\n",
       " 'russia',\n",
       " 'rufiyaa',\n",
       " 'maldives',\n",
       " 'rupee',\n",
       " 'india',\n",
       " 'rupee',\n",
       " 'pakistan',\n",
       " 'rupiah',\n",
       " 'indonesia',\n",
       " 'shilling',\n",
       " 'uganda',\n",
       " 'som',\n",
       " 'uzbekistan',\n",
       " 'somoni',\n",
       " 'tajikistan',\n",
       " 'special',\n",
       " 'drawing',\n",
       " 'rights',\n",
       " 'international',\n",
       " 'monetary',\n",
       " 'fund',\n",
       " 'taka',\n",
       " 'bangladesh',\n",
       " 'tala',\n",
       " 'western',\n",
       " 'samoa',\n",
       " 'tenge',\n",
       " 'kazakhstan',\n",
       " 'tugrik',\n",
       " 'mongolia',\n",
       " 'vatu',\n",
       " 'vanuatu',\n",
       " 'won',\n",
       " 'korea,',\n",
       " 'south',\n",
       " 'yen',\n",
       " 'japan',\n",
       " 'zloty',\n",
       " 'poland',\n",
       " 'hundred',\n",
       " 'denominations',\n",
       " 'thousand',\n",
       " 'million',\n",
       " 'billion',\n",
       " 'trillion',\n",
       " 'date',\n",
       " 'time',\n",
       " 'related',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'annum',\n",
       " 'year',\n",
       " 'yearly',\n",
       " 'quarter',\n",
       " 'quarterly',\n",
       " 'qtr',\n",
       " 'month',\n",
       " 'monthly',\n",
       " 'week',\n",
       " 'weekly',\n",
       " 'day',\n",
       " 'daily',\n",
       " 'january',\n",
       " 'calendar',\n",
       " 'february',\n",
       " 'march',\n",
       " 'april',\n",
       " 'may',\n",
       " 'june',\n",
       " 'july',\n",
       " 'august',\n",
       " 'september',\n",
       " 'october',\n",
       " 'november',\n",
       " 'december',\n",
       " 'jan',\n",
       " 'feb',\n",
       " 'mar',\n",
       " 'apr',\n",
       " 'may',\n",
       " 'jun',\n",
       " 'jul',\n",
       " 'aug',\n",
       " 'sep',\n",
       " 'sept',\n",
       " 'oct',\n",
       " 'nov',\n",
       " 'dec',\n",
       " 'monday',\n",
       " 'tuesday',\n",
       " 'wednesday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'saturday',\n",
       " 'sunday',\n",
       " 'one',\n",
       " 'numbers',\n",
       " 'two',\n",
       " 'three',\n",
       " 'four',\n",
       " 'five',\n",
       " 'six',\n",
       " 'seven',\n",
       " 'eight',\n",
       " 'nine',\n",
       " 'ten',\n",
       " 'eleven',\n",
       " 'twelve',\n",
       " 'thirteen',\n",
       " 'fourteen',\n",
       " 'fifteen',\n",
       " 'sixteen',\n",
       " 'seventeen',\n",
       " 'eighteen',\n",
       " 'nineteen',\n",
       " 'twenty',\n",
       " 'thirty',\n",
       " 'forty',\n",
       " 'fifty',\n",
       " 'sixty',\n",
       " 'seventy',\n",
       " 'eighty',\n",
       " 'ninety',\n",
       " 'first',\n",
       " 'second',\n",
       " 'third',\n",
       " 'fourth',\n",
       " 'fifth',\n",
       " 'sixth',\n",
       " 'seventh',\n",
       " 'eighth',\n",
       " 'ninth',\n",
       " 'tenth',\n",
       " 'i',\n",
       " 'roman',\n",
       " 'numerals',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'iv',\n",
       " 'v',\n",
       " 'vi',\n",
       " 'vii',\n",
       " 'viii',\n",
       " 'ix',\n",
       " 'x',\n",
       " 'xi',\n",
       " 'xii',\n",
       " 'xiii',\n",
       " 'xiv',\n",
       " 'xv',\n",
       " 'xvi',\n",
       " 'xvii',\n",
       " 'xviii',\n",
       " 'xix',\n",
       " 'xx',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'all',\n",
       " 'am',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " 'she',\n",
       " 'should',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'with',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'would',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " ...]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stopwords = [word for word in custom_stopwords if word!='|']\n",
    "custom_stopwords = [word.lower() for word in custom_stopwords]\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d6f2d",
   "metadata": {},
   "source": [
    "### Steps for preprocessing the data:\n",
    "Remove the special characters <br>\n",
    "Convert the entire data to upper or lower case <br>\n",
    "Tokenize the data <br>\n",
    "Remove stopwords <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c5d5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(original_text):\n",
    "    if(pd.notnull(original_text)):     \n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    #     pattern = r'[^\\w\\s]'\n",
    "        preprocessed_text = re.sub(pattern,' ',original_text)\n",
    "        preprocessed_text = re.sub('\\n',' ',preprocessed_text)\n",
    "        preprocessed_text = re.sub('\\xa0',' ',preprocessed_text)        \n",
    "        return preprocessed_text    \n",
    "\n",
    "def convert_lowercase(original_text):\n",
    "    if pd.notnull(original_text):\n",
    "        return original_text.lower()\n",
    "\n",
    "def tokenization(original_text):\n",
    "    if pd.notnull(original_text):\n",
    "        return word_tokenize(original_text)        # returns a list of tokens\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    if token_list is not None:\n",
    "        stop_words = stopwords.words('english')\n",
    "        return [word for word in token_list if word not in custom_stopwords+stop_words]\n",
    "    \n",
    "# combine all the above steps into one function\n",
    "\n",
    "def preprocessing(original_text):\n",
    "    return remove_stopwords(tokenization(convert_lowercase(remove_special_characters(original_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4c304b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a new column to store the cleaned list of words\n",
    "\n",
    "final['processed_data'] = final['ARTICLE_TEXT'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77402406",
   "metadata": {},
   "source": [
    "### NLP - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "91491249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative-words.txt', 'positive-words.txt']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need the dictionary in this section\n",
    "\n",
    "dict_folder_path = '../MasterDictionary'\n",
    "\n",
    "os.listdir(dict_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c58cba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making list of positive and negative words from the files \n",
    "\n",
    "neg_words_filepath = os.path.join(dict_folder_path,'negative-words.txt')\n",
    "with open(neg_words_filepath,'r') as file:\n",
    "    negative_words = file.read().split()\n",
    "    \n",
    "pos_words_filepath = os.path.join(dict_folder_path,'positive-words.txt')\n",
    "with open(pos_words_filepath,'r') as file:\n",
    "    positive_words = file.read().split()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dc39f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive score\n",
    "\n",
    "def calculate_positive_score(list_of_words):\n",
    "    if list_of_words is not None:\n",
    "        return sum(word in positive_words for word in list_of_words)\n",
    "    \n",
    "# negative score\n",
    "\n",
    "def calculate_negative_score(list_of_words):\n",
    "    if list_of_words is not None:\n",
    "        return sum(word in negative_words for word in list_of_words)\n",
    "    \n",
    "# polarity score \n",
    "    \n",
    "def calculate_polarity(pos_score, neg_score):\n",
    "    if pd.notnull(pos_score):\n",
    "        return (pos_score-neg_score)/((pos_score+neg_score)+0.000001)\n",
    "    \n",
    "# subjectivity score \n",
    "    \n",
    "def calculate_subjectivity(pos_score, neg_score, processed_data_list):\n",
    "    if pd.notnull(pos_score):\n",
    "        total_words = len(processed_data_list)\n",
    "        return (pos_score+neg_score)/(total_words+0.000001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "de0b689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['POSITIVE SCORE'] = final['processed_data'].apply(calculate_positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3d565183",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['NEGATIVE SCORE'] = final['processed_data'].apply(calculate_negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "57526d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis =1 denotes the row-wise operations\n",
    "\n",
    "final['POLARITY SCORE'] = final.apply(lambda each_row: calculate_polarity(each_row['POSITIVE SCORE'], each_row['NEGATIVE SCORE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "18f80352",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['SUBJECTIVITY SCORE'] = final.apply(lambda each_row: calculate_subjectivity(each_row['POSITIVE SCORE'], each_row['NEGATIVE SCORE'], each_row['processed_data']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b13106",
   "metadata": {},
   "source": [
    "### Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b03d55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average sentence length\n",
    "\n",
    "def calculate_avg_sent_len(article_text):\n",
    "    if pd.notnull(article_text):\n",
    "        cleaned_text = remove_special_characters(article_text)    # removes characters such as \" , \" so that it wont be counted when counting the words\n",
    "        num_of_words = len(word_tokenize(cleaned_text))           \n",
    "        num_of_sents = len(sent_tokenize(article_text))\n",
    "        if num_of_sents>0:\n",
    "            return num_of_words/num_of_sents\n",
    "        else:\n",
    "            return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dcf28c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final['AVG SENTENCE LENGTH'] = final['ARTICLE_TEXT'].apply(calculate_avg_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee91cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syllables\n",
      "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
      "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
      "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
      "                                              0.0/939.3 kB ? eta -:--:--\n",
      "     --------------                        358.4/939.3 kB 10.9 MB/s eta 0:00:01\n",
      "     -------------------------------       809.0/939.3 kB 10.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 939.3/939.3 kB 9.9 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata<6.0.0,>=5.1.0 (from syllables)\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources<6.0.0,>=5.10.1 (from cmudict<2.0.0,>=1.0.11->syllables)\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<6.0.0,>=5.1.0->syllables)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: zipp, importlib-resources, importlib-metadata, cmudict, syllables\n",
      "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.12.0 syllables-1.0.7 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "# the percentage of complex words\n",
    "# we will determine if the word is complex on the basis of syllables\n",
    "# if the number of syllables exceed 2 we can say it is complex\n",
    "\n",
    "# !pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "577b3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "\n",
    "def calculate_complex_percentage(article_text):\n",
    "    if pd.notnull(article_text):\n",
    "        cleaned_text = remove_special_characters(article_text)    # removes characters such as \" , \" so that it wont be counted when counting the words\n",
    "        list_of_words = word_tokenize(cleaned_text)\n",
    "        num_of_words = len(list_of_words)  \n",
    "        if num_of_words>0:\n",
    "            num_of_complex_words = sum(syllables.estimate(word)>=3 for word in list_of_words)\n",
    "            return (num_of_complex_words/num_of_words)*100\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "        \n",
    "# syllables.estimate(word) counts the number of syllables in the word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "24c2eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['PERCENTAGE OF COMPLEX WORDS'] = final['ARTICLE_TEXT'].apply(calculate_complex_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e152eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fog index\n",
    "\n",
    "def calculate_fog_index(average_words_per_sentence, percentage_complex_words):\n",
    "    return 0.4 * (average_words_per_sentence + percentage_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f385c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['FOG INDEX'] = final.apply(lambda eachrow : calculate_fog_index(eachrow['AVG SENTENCE LENGTH'],eachrow['PERCENTAGE OF COMPLEX WORDS']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "97aed4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of words per sentence is same as avg sentence length\n",
    "\n",
    "final['AVG NUMBER OF WORDS PER SENTENCE'] = final['ARTICLE_TEXT'].apply(calculate_avg_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "fa78b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex word count \n",
    "\n",
    "def calculate_complex_words(article_text):\n",
    "    if pd.notnull(article_text):\n",
    "        cleaned_text = remove_special_characters(article_text)    # removes characters such as \" , \" so that it wont be counted when counting the words\n",
    "        list_of_words = word_tokenize(cleaned_text)\n",
    "        return sum(syllables.estimate(word)>=3 for word in list_of_words)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6e009e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['COMPLEX WORD COUNT'] = final['ARTICLE_TEXT'].apply(calculate_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c75ae661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cleaned words\n",
    "\n",
    "def calculate_word_count(article_text):\n",
    "    if pd.notnull(article_text):\n",
    "        cleaned_text = convert_lowercase(remove_special_characters(article_text))        \n",
    "        list_of_words = word_tokenize(cleaned_text)\n",
    "        stop_words = stopwords.words('english')\n",
    "        list_of_words = [word for word in list_of_words if word not in stop_words]\n",
    "        return len(list_of_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "cff8933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['WORD COUNT'] = final['ARTICLE_TEXT'].apply(calculate_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5cdade8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable per word\n",
    "\n",
    "def syllable_count(article_text):\n",
    "    if pd.notnull(article_text):       \n",
    "        cleaned_text = convert_lowercase(remove_special_characters(article_text))   \n",
    "        list_of_words = word_tokenize(cleaned_text)\n",
    "        stop_words = stopwords.words('english')\n",
    "        list_of_words = [word for word in list_of_words if word not in stop_words]\n",
    "        syllable_count_list = [syllables.estimate(word) for word in list_of_words]\n",
    "        return syllable_count_list    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "51b0e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['SYLLABLE PER WORD'] = final['ARTICLE_TEXT'].apply(syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a009ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg word length\n",
    "\n",
    "def calculate_avg_word_length(word_list):\n",
    "    if word_list is not None:\n",
    "        total_words = len(word_list)\n",
    "        total_characters = sum(len(word) for word in word_list)\n",
    "\n",
    "        if total_words > 0:\n",
    "            average_length = total_characters / total_words\n",
    "            return average_length\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1462127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['AVG WORD LENGTH'] = final['processed_data'].apply(calculate_avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9943562c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>ARTICLE_TITLE</th>\n",
       "      <th>ARTICLE_TEXT</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“If anything kills over 10 million people in t...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>29.824561</td>\n",
       "      <td>21.049825</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>[3, 1, 1, 2, 2, 1, 3, 2, 3, 2, 2, 1, 3, 3, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.701754</td>\n",
       "      <td>[kills, 10, people, decades, highly, infectiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>21.348315</td>\n",
       "      <td>17.439326</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>[2, 1, 4, 2, 3, 3, 3, 2, 1, 5, 2, 2, 3, 2, 3, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.322581</td>\n",
       "      <td>[human, minds, fascination, carrying, potentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>AI is rapidly evolving in the employment secto...</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>32.236842</td>\n",
       "      <td>20.494737</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[1, 3, 3, 3, 2, 5, 2, 3, 3, 3, 3, 4, 4, 3, 2, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.407895</td>\n",
       "      <td>[rapidly, evolving, employment, sector, matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>“Anything that could give rise to smarter-than...</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>19.565217</td>\n",
       "      <td>17.026087</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>[3, 1, 1, 1, 2, 2, 5, 1, 4, 5, 1, 3, 4, 4, 2, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>[give, rise, smarter, human, intelligence, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>“Machine intelligence is the last invention th...</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>17.507692</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[3, 5, 1, 3, 4, 2, 1, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>[machine, intelligence, invention, humanity, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>Blockchain for Payments</td>\n",
       "      <td>between having a tight budget and being seriou...</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>17.428571</td>\n",
       "      <td>7.377049</td>\n",
       "      <td>9.922248</td>\n",
       "      <td>17.428571</td>\n",
       "      <td>9.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[1, 2, 3, 2, 1, 2, 4, 2, 2, 1, 1, 2, 2, 2, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.135135</td>\n",
       "      <td>[tight, budget, stretched, thin, dangerously, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>The future of Investing</td>\n",
       "      <td>An investment is a resource or thing procured ...</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>[3, 3, 1, 3, 4, 3, 1, 4, 6, 3, 1, 2, 3, 3, 4, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.225806</td>\n",
       "      <td>[investment, resource, thing, procured, object...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>Big Data Analytics in Healthcare</td>\n",
       "      <td>Quality and affordable healthcare is a vision ...</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>29.310345</td>\n",
       "      <td>17.524138</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[3, 4, 3, 2, 3, 2, 2, 1, 1, 3, 3, 1, 3, 5, 4, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.083333</td>\n",
       "      <td>[quality, affordable, healthcare, vision, gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>Business Analytics In The Healthcare Industry</td>\n",
       "      <td>Analytics is a statistical scientific process ...</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>38.297872</td>\n",
       "      <td>24.719149</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>[4, 4, 3, 2, 4, 3, 3, 2, 1, 2, 3, 4, 2, 1, 4, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.379310</td>\n",
       "      <td>[analytics, statistical, scientific, process, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "      <td>To begin with I shall first like to explain wh...</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.145833</td>\n",
       "      <td>12.858333</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 1, 2, 2, 2, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.225000</td>\n",
       "      <td>[begin, explain, big, data, important, lives, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                      ARTICLE_TITLE  \\\n",
       "0        37                                                NaN   \n",
       "1        38   What if the Creation is Taking Over the Creator?   \n",
       "2        39  What Jobs Will Robots Take From Humans in The ...   \n",
       "3        40  Will Machine Replace The Human in the Future o...   \n",
       "4        41                Will AI Replace Us or Work With Us?   \n",
       "..      ...                                                ...   \n",
       "109     146                            Blockchain for Payments   \n",
       "110     147                            The future of Investing   \n",
       "111     148                   Big Data Analytics in Healthcare   \n",
       "112     149      Business Analytics In The Healthcare Industry   \n",
       "113     150  Challenges and Opportunities of Big Data in He...   \n",
       "\n",
       "                                          ARTICLE_TEXT  \\\n",
       "0    “If anything kills over 10 million people in t...   \n",
       "1    Human minds, a fascination in itself carrying ...   \n",
       "2    AI is rapidly evolving in the employment secto...   \n",
       "3    “Anything that could give rise to smarter-than...   \n",
       "4    “Machine intelligence is the last invention th...   \n",
       "..                                                 ...   \n",
       "109  between having a tight budget and being seriou...   \n",
       "110  An investment is a resource or thing procured ...   \n",
       "111  Quality and affordable healthcare is a vision ...   \n",
       "112  Analytics is a statistical scientific process ...   \n",
       "113  To begin with I shall first like to explain wh...   \n",
       "\n",
       "                                                   URL  POSITIVE SCORE  \\\n",
       "0    https://insights.blackcoffer.com/ai-in-healthc...             3.0   \n",
       "1    https://insights.blackcoffer.com/what-if-the-c...             8.0   \n",
       "2    https://insights.blackcoffer.com/what-jobs-wil...             7.0   \n",
       "3    https://insights.blackcoffer.com/will-machine-...             6.0   \n",
       "4    https://insights.blackcoffer.com/will-ai-repla...             1.0   \n",
       "..                                                 ...             ...   \n",
       "109  https://insights.blackcoffer.com/blockchain-fo...             1.0   \n",
       "110  https://insights.blackcoffer.com/the-future-of...             1.0   \n",
       "111  https://insights.blackcoffer.com/big-data-anal...             4.0   \n",
       "112  https://insights.blackcoffer.com/business-anal...             2.0   \n",
       "113  https://insights.blackcoffer.com/challenges-an...             2.0   \n",
       "\n",
       "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              11.0       -0.571429            0.245614            22.800000   \n",
       "1               5.0        0.230769            0.419355            22.250000   \n",
       "2               2.0        0.555555            0.118421            19.000000   \n",
       "3               0.0        1.000000            0.285714            23.000000   \n",
       "4               0.0        0.999999            0.200000            13.000000   \n",
       "..              ...             ...                 ...                  ...   \n",
       "109             2.0       -0.333333            0.081081            17.428571   \n",
       "110             0.0        0.999999            0.032258            23.333333   \n",
       "111             2.0        0.333333            0.250000            14.500000   \n",
       "112             0.0        1.000000            0.068966            23.500000   \n",
       "113             2.0        0.000000            0.050000            16.000000   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                      29.824561  21.049825                         22.800000   \n",
       "1                      21.348315  17.439326                         22.250000   \n",
       "2                      32.236842  20.494737                         19.000000   \n",
       "3                      19.565217  17.026087                         23.000000   \n",
       "4                      30.769231  17.507692                         13.000000   \n",
       "..                           ...        ...                               ...   \n",
       "109                     7.377049   9.922248                         17.428571   \n",
       "110                    30.000000  21.333333                         23.333333   \n",
       "111                    29.310345  17.524138                         14.500000   \n",
       "112                    38.297872  24.719149                         23.500000   \n",
       "113                    16.145833  12.858333                         16.000000   \n",
       "\n",
       "     COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                  34.0        68.0   \n",
       "1                  19.0        48.0   \n",
       "2                  49.0        93.0   \n",
       "3                   9.0        28.0   \n",
       "4                   4.0         8.0   \n",
       "..                  ...         ...   \n",
       "109                 9.0        60.0   \n",
       "110                21.0        36.0   \n",
       "111                17.0        30.0   \n",
       "112                18.0        29.0   \n",
       "113                31.0       107.0   \n",
       "\n",
       "                                     SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0    [3, 1, 1, 2, 2, 1, 3, 2, 3, 2, 2, 1, 3, 3, 1, ...                NaN   \n",
       "1    [2, 1, 4, 2, 3, 3, 3, 2, 1, 5, 2, 2, 3, 2, 3, ...                NaN   \n",
       "2    [1, 3, 3, 3, 2, 5, 2, 3, 3, 3, 3, 4, 4, 3, 2, ...                NaN   \n",
       "3    [3, 1, 1, 1, 2, 2, 5, 1, 4, 5, 1, 3, 4, 4, 2, ...                NaN   \n",
       "4                             [3, 5, 1, 3, 4, 2, 1, 1]                NaN   \n",
       "..                                                 ...                ...   \n",
       "109  [1, 2, 3, 2, 1, 2, 4, 2, 2, 1, 1, 2, 2, 2, 1, ...                NaN   \n",
       "110  [3, 3, 1, 3, 4, 3, 1, 4, 6, 3, 1, 2, 3, 3, 4, ...                NaN   \n",
       "111  [3, 4, 3, 2, 3, 2, 2, 1, 1, 3, 3, 1, 3, 5, 4, ...                NaN   \n",
       "112  [4, 4, 3, 2, 4, 3, 3, 2, 1, 2, 3, 4, 2, 1, 4, ...                NaN   \n",
       "113  [2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 1, 2, 2, 2, 1, ...                NaN   \n",
       "\n",
       "     AVG WORD LENGTH                                     processed_data  \n",
       "0           7.701754  [kills, 10, people, decades, highly, infectiou...  \n",
       "1           7.322581  [human, minds, fascination, carrying, potentia...  \n",
       "2           7.407895  [rapidly, evolving, employment, sector, matter...  \n",
       "3           7.333333  [give, rise, smarter, human, intelligence, for...  \n",
       "4           8.000000  [machine, intelligence, invention, humanity, m...  \n",
       "..               ...                                                ...  \n",
       "109         6.135135  [tight, budget, stretched, thin, dangerously, ...  \n",
       "110         7.225806  [investment, resource, thing, procured, object...  \n",
       "111         8.083333  [quality, affordable, healthcare, vision, gove...  \n",
       "112         8.379310  [analytics, statistical, scientific, process, ...  \n",
       "113         6.225000  [begin, explain, big, data, important, lives, ...  \n",
       "\n",
       "[114 rows x 18 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f546c",
   "metadata": {},
   "source": [
    "### OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b9d60e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desirable format for output\n",
    "\n",
    "final.drop(columns = ['ARTICLE_TITLE','ARTICLE_TEXT','processed_data'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b86eedfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>22.80</td>\n",
       "      <td>29.824561</td>\n",
       "      <td>21.049825</td>\n",
       "      <td>22.80</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>[3, 1, 1, 2, 2, 1, 3, 2, 3, 2, 2, 1, 3, 3, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>22.25</td>\n",
       "      <td>21.348315</td>\n",
       "      <td>17.439326</td>\n",
       "      <td>22.25</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>[2, 1, 4, 2, 3, 3, 3, 2, 1, 5, 2, 2, 3, 2, 3, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>19.00</td>\n",
       "      <td>32.236842</td>\n",
       "      <td>20.494737</td>\n",
       "      <td>19.00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[1, 3, 3, 3, 2, 5, 2, 3, 3, 3, 3, 4, 4, 3, 2, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.407895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.565217</td>\n",
       "      <td>17.026087</td>\n",
       "      <td>23.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>[3, 1, 1, 1, 2, 2, 5, 1, 4, 5, 1, 3, 4, 4, 2, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>13.00</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>17.507692</td>\n",
       "      <td>13.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[3, 5, 1, 3, 4, 2, 1, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...             3.0   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...             8.0   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...             7.0   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...             6.0   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...             1.0   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0            11.0       -0.571429            0.245614                22.80   \n",
       "1             5.0        0.230769            0.419355                22.25   \n",
       "2             2.0        0.555555            0.118421                19.00   \n",
       "3             0.0        1.000000            0.285714                23.00   \n",
       "4             0.0        0.999999            0.200000                13.00   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                    29.824561  21.049825                             22.80   \n",
       "1                    21.348315  17.439326                             22.25   \n",
       "2                    32.236842  20.494737                             19.00   \n",
       "3                    19.565217  17.026087                             23.00   \n",
       "4                    30.769231  17.507692                             13.00   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                34.0        68.0   \n",
       "1                19.0        48.0   \n",
       "2                49.0        93.0   \n",
       "3                 9.0        28.0   \n",
       "4                 4.0         8.0   \n",
       "\n",
       "                                   SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0  [3, 1, 1, 2, 2, 1, 3, 2, 3, 2, 2, 1, 3, 3, 1, ...                NaN   \n",
       "1  [2, 1, 4, 2, 3, 3, 3, 2, 1, 5, 2, 2, 3, 2, 3, ...                NaN   \n",
       "2  [1, 3, 3, 3, 2, 5, 2, 3, 3, 3, 3, 4, 4, 3, 2, ...                NaN   \n",
       "3  [3, 1, 1, 1, 2, 2, 5, 1, 4, 5, 1, 3, 4, 4, 2, ...                NaN   \n",
       "4                           [3, 5, 1, 3, 4, 2, 1, 1]                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         7.701754  \n",
       "1         7.322581  \n",
       "2         7.407895  \n",
       "3         7.333333  \n",
       "4         8.000000  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "dc085af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('../Final_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3f28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
